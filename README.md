# Meetup Lokale LLMs ğŸš€

## ğŸ“„ Abstract
In diesem Meetup beschÃ¤ftigen wir uns mit der Nutzung **lokaler Large Language Models (LLMs)** auf dem eigenen Rechner. Dazu setzen wir **Ollama** als LLM-Backend ein und nutzen **Gradio**, um eine einfache Chat-UI bereitzustellen. ZusÃ¤tzlich testen wir die **Ollama API** mit speziell geschriebenen **Batch-Skripten**, die verschiedene Interaktionen ermÃ¶glichen. Ziel ist es, eine Umgebung zu schaffen, in der wir interaktiv mit einem lokalen LLM kommunizieren kÃ¶nnen, ohne auf Cloud-Dienste angewiesen zu sein.

## ğŸ”§ Voraussetzungen
Damit das Projekt ausgefÃ¼hrt werden kann, benÃ¶tigst du folgende Komponenten auf deinem System:

- **ğŸ Python 3.8+** (am besten die neueste Version)
- **ğŸ“¦ Pip** (Python-Paketmanager, kommt mit Python)
- **ğŸ’» Ollama** (lokales LLM-Backend, kann [hier](https://ollama.com) installiert werden)
- **ğŸŒ Internetverbindung** (zum einmaligen Installieren der AbhÃ¤ngigkeiten)
- **ğŸ“œ Windows CMD oder eine andere Shell**, um die `.bat`-Skripte auszufÃ¼hren

## ğŸ“‚ Projektstruktur
```
ğŸ“ meetup-local-llms
â”œâ”€â”€ ğŸ“ slides
|   â”œâ”€â”€ 250311_meetup_workshop_local_llms_ollama_msty.pdf 
â”œâ”€â”€ ollama-cmd.bat          # Batch-Skript zum direkten Start von Ollama
â”œâ”€â”€ ollama-api.bat          # Batch-Skript zur Nutzung der Ollama API
â”œâ”€â”€ ollama-chat.py          # GradioUI-Chatbot-Skript fÃ¼r Ollama via API
â”œâ”€â”€ start-ollama-chatui.bat # Batch-Skript zum Start der Gradio-UI
â”œâ”€â”€ requirements.txt        # Liste der benÃ¶tigten Python-Pakete
â”œâ”€â”€ README.md               # Dokumentation
```

## ğŸš€ Installation & AusfÃ¼hrung
### **1ï¸âƒ£ Python & Ollama installieren**
Falls Python noch nicht installiert ist, kannst du es [hier herunterladen](https://www.python.org/downloads/). 
Wie du Ollama und alles weitere installierst, findest du in ğŸ“ slides.

### **2ï¸âƒ£ Repository klonen oder Dateien herunterladen**
Falls du das Projekt von GitHub holst:
```sh
git clone https://github.com/ScaDS/scads-ai-meetup-local-llms.git
cd scads-ai-meetup-local-llms
```
Wie du Git installieren kannst findest du [hier](https://git-scm.com/). 


### **3ï¸âƒ£ Starten Ã¼ber die Batch-Dateien**
#### **Option 1: Direktes Arbeiten mit Ollama im Terminal**
Doppelklicke auf `ollama-cmd.bat`, um eine Terminal-Sitzung mit Ollama zu starten.

#### **Option 2: API-Nutzung testen**
Doppelklicke auf `ollama-api.bat`, um:
âœ… Eine Eingabeaufforderung zur API-Nutzung zu Ã¶ffnen
âœ… Die IP des Ollama-Servers abzufragen
âœ… Eine Frage an die API zu senden

#### **Option 3: Gradio UI starten**
Doppelklicke auf `start-ollama-chatui.bat`, um:
âœ… **Die benÃ¶tigten AbhÃ¤ngigkeiten zu installieren**
âœ… **Den Gradio-Chatbot zu starten**

Alternativ kannst du alles manuell ausfÃ¼hren:
```sh
pip install -r requirements.txt
python chatbot.py
```

### **4ï¸âƒ£ Chatbot im Browser Ã¶ffnen**
Sobald Gradio lÃ¤uft, kannst du den Chatbot in deinem Browser Ã¶ffnen:  
**[http://127.0.0.1:7860](http://127.0.0.1:7860)**

Falls der Port bereits belegt ist, kannst du Gradio mit einem anderen Port starten:
```sh
python chatbot.py --server-port 7861
```

## ğŸ“œ Weitere Hinweise
- **âš¡ Falls `start-ollama-chatui.bat` nicht funktioniert**, stelle sicher, dass Python korrekt installiert ist.
- **ğŸ”„ Falls du die AbhÃ¤ngigkeiten erneut installieren mÃ¶chtest**, lÃ¶sche einfach die `venv`-Umgebung (falls vorhanden) und starte `start-ollama-chatui.bat` erneut.
- **ğŸ“¢ Falls du nur mit der API arbeiten willst, kannst du `ollama-api.bat` nutzen**, um manuell Anfragen zu senden.

## ğŸ‰ Viel SpaÃŸ beim Experimentieren mit lokalen LLMs! ğŸ¤–ğŸ’¬

